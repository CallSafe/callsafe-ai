{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OM4ByVuU9r6R",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Korean Text classification with KoBERT</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q30zE2Xy-ipp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Summary of the project**\n",
    "\n",
    "In Korea, even though there are many research conducted being conducted on voice phishing, it remains a real case problem that technology such as artificial intelligence can tackle. Through a previous project conducted, we created a dataset containing phone call conversation transcripts and general conversation text data. This voice phishing dataset has two different class which are **voice phishing** (represented as \"1\") and **non-voice phishing** (represented as \"0\").\n",
    "\n",
    "Using this dataset with state-of-the-art (SOTA) pre-trained word embedding [KoBERT](https://github.com/SKTBrain/KoBERT), we will perform NLP task such as text classification to build binary classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z48RIrXt-27-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Aim of the project**\n",
    "In this project, we aim to build binary classification models capable to determine whether the inputted Korean conversation text is voice phishing (\"1\") or non-voice phishing (\"0\") related text.\n",
    "\n",
    "The API used are Tensorflow for BERT model and Pytorch for KoBERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZWbz7tr_D4e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Desired outputs of the project**\n",
    "From the trained models, we expect to achieve great classification performance on this voice phishing dataset such as the model tells us if a conversation is harmful or not harmful.\n",
    "At the end of this project, we will look at the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U5oXwtUIUZx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the binary classification model with KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMY5U0umGrhJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installing the common needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SygClswK-r1",
    "outputId": "c385330a-d2e0-425f-8bc4-a0d127f6e54a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to c:\\users\\hye42\\appdata\\local\\temp\\pip-req-build-_sr58v90\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit fcd729f2f4b37858f333597c0782388ada51eb5f\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting accelerate (from kobert==0.2.4)\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/9f/1c/a17fb513aeb684fb83bef5f395910f53103ab30308bbdd77fd66d6698c46/accelerate-1.9.0-py3-none-any.whl.metadata\n",
      "  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets (from kobert==0.2.4)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/eb/62/eb8157afb21bd229c864521c1ab4fa8e9b4f1b06bafdd8c4668a7a31b5dd/datasets-4.0.0-py3-none-any.whl.metadata\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting gluonnlp<=0.10.0,>=0.6.0 (from kobert==0.2.4)\n",
      "  Using cached gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting mxnet<=1.7.0.post2,>=1.4.0 (from kobert==0.2.4)\n",
      "  Obtaining dependency information for mxnet<=1.7.0.post2,>=1.4.0 from https://files.pythonhosted.org/packages/2a/4f/fc1adda6d1ceef5c9b3921e6c6d208b86b1b3ed61686a84c1b1bcc821798/mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached mxnet-1.7.0.post2-py2.py3-none-win_amd64.whl.metadata (402 bytes)\n",
      "Collecting onnxruntime (from kobert==0.2.4)\n",
      "  Obtaining dependency information for onnxruntime from https://files.pythonhosted.org/packages/a8/01/e536397b03e4462d3260aee5387e6f606c8fa9d2b20b1728f988c3c72891/onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached onnxruntime-1.22.1-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hye42\\korean_voice_phishing_detection\\venv\\lib\\site-packages (from kobert==0.2.4) (6.30.2)\n",
      "Collecting sentencepiece<=0.1.96,>=0.1.6 (from kobert==0.2.4)\n",
      "  Using cached sentencepiece-0.1.96.tar.gz (508 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\hye42\\korean_voice_phishing_detection\\venv\\lib\\site-packages (from kobert==0.2.4) (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\hye42\\korean_voice_phishing_detection\\venv\\lib\\site-packages (from kobert==0.2.4) (4.51.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hye42\\korean_voice_phishing_detection\\venv\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.4) (2.2.6)\n",
      "Collecting cython (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.4)\n",
      "  Obtaining dependency information for cython from https://files.pythonhosted.org/packages/a2/50/0aa65be5a4ab65bde3224b8fd23ed795f699d1e724ac109bb0a32036b82d/cython-3.1.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached cython-3.1.2-cp311-cp311-win_amd64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\hye42\\korean_voice_phishing_detection\\venv\\lib\\site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.4) (25.0)\n",
      "Collecting numpy>=1.16.0 (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.4)\n",
      "  Using cached numpy-1.16.6.zip (5.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' 'C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-req-build-_sr58v90'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [27 lines of output]\n",
      "  Running from numpy source directory.\n",
      "  <string>:394: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "  C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-install-vfe4ptdt\\numpy_c2c25a7202384645a1163a216c1a5eb7\\numpy\\distutils\\misc_util.py:476: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "    return is_string(s) and ('*' in s or '?' is s)\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\hye42\\Korean_Voice_Phishing_Detection\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "      main()\n",
      "    File \"C:\\Users\\hye42\\Korean_Voice_Phishing_Detection\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\hye42\\Korean_Voice_Phishing_Detection\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "      return hook(metadata_directory, config_settings)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-build-env-a79wi_5x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 374, in prepare_metadata_for_build_wheel\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-build-env-a79wi_5x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "      super().run_setup(setup_script=setup_script)\n",
      "    File \"C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-build-env-a79wi_5x\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 419, in <module>\n",
      "    File \"<string>\", line 398, in setup_package\n",
      "    File \"C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-install-vfe4ptdt\\numpy_c2c25a7202384645a1163a216c1a5eb7\\numpy\\distutils\\__init__.py\", line 6, in <module>\n",
      "      from . import ccompiler\n",
      "    File \"C:\\Users\\hye42\\AppData\\Local\\Temp\\pip-install-vfe4ptdt\\numpy_c2c25a7202384645a1163a216c1a5eb7\\numpy\\distutils\\ccompiler.py\", line 111, in <module>\n",
      "      replace_method(CCompiler, 'find_executables', CCompiler_find_executables)\n",
      "                     ^^^^^^^^^\n",
      "  NameError: name 'CCompiler' is not defined. Did you mean: 'ccompiler'?\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# dowload and install KoBERT as a python package\n",
    "# this commande will install the requirted package at the same time\n",
    "  # gluonnlp >= 0.6.0\n",
    "  # mxnet >= 1.4.0\n",
    "  # onnxruntime >= 0.3.0\n",
    "  # sentencepiece >= 0.1.6\n",
    "  # torch >= 1.7.0\n",
    "  # transformers >= 4.8.1\n",
    "\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl-QdJx5O30T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import all the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kZM6H2ZhM4rE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gluonnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgluonnlp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gluonnlp'"
     ]
    }
   ],
   "source": [
    "## importing the required packages\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFFUZg92HcCj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3MdTD7jN7zN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## importing KoBERT functions\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wrZ2zmpOfEx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## import transformers functions\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLMyT454u9KP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Configure the GPU  device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfQAKaHmTX9v",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok-GzRAgTd28",
    "outputId": "59326451-c380-4a5e-cb4f-2c74d7dacb8c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since we are using Colab, we will provide a test to check if environment is \n",
    "colab or not so that the data can also be imported in case this jupyter file is \n",
    "ran on local machine and not on colab\n",
    "\"\"\"\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  ## mount the google drive\n",
    "  from google.colab import drive\n",
    "  drive.mount('drive')\n",
    "  # move to the directory where dataset is saved\n",
    "  %cd drive/My\\ Drive/Colab\\ Notebooks/\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "vdZBdvWvnbfB",
    "outputId": "e03a5fc6-504b-45dd-a4aa-7d78ba71d2b8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "dataset = pd.read_csv('KorCCViD_v1.3_fullcleansed.csv').sample(frac=1.0)\n",
    "dataset.sample(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-P7hdv9mdt6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data transformation and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bB_0ae0MkgOT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## transform our train set and test set into tsv file to usedd into KoBERT\n",
    "# train_tsv = nlp.data.TSVDataset('KorCCViD_v1.3_fullcleansed.csv')\n",
    "# train_tsv = nlp.data.TSVDataset('KorCCViD_v1.3_fullcleansed.csv')\n",
    "\n",
    "dataset_tsv = []\n",
    "for text, label in zip(dataset['Transcript'], dataset['Label']):\n",
    "    data = []\n",
    "    data.append(text)\n",
    "    data.append(str(label))\n",
    "\n",
    "    dataset_tsv.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UXD_FjO2qXrU",
    "outputId": "40720195-8f80-4ea8-85d5-77c35ed314a7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_tsv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrrMMwDSmjes",
    "outputId": "8ef4c75d-c672-4cca-d597-05c56b5d857c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train set and test set\n",
    "\n",
    "# train_set, test_set = train_test_split(dataset_tsv, \n",
    "#                                test_size=0.3, \n",
    "#                                random_state=42, \n",
    "#                                shuffle=True)\n",
    "# print(f\"Numbers of train instances by class: {len(train_set)}\")\n",
    "# print(f\"Numbers of test instances by class: {len(test_set)}\")\n",
    "\n",
    "train_set, val_set = train_test_split(dataset_tsv, \n",
    "                               test_size=0.2, \n",
    "                               random_state=42, \n",
    "                               shuffle=True)\n",
    "\n",
    "# train_set, val_set = train_test_split(train_set, \n",
    "#                                test_size=0.2, \n",
    "#                                random_state=42, \n",
    "#                                shuffle=True)\n",
    "print(f\"Numbers of train instances by class: {len(train_set)}\")\n",
    "print(f\"Numbers of val instances by class: {len(val_set)}\")\n",
    "# print(f\"Numbers of test instances by class: {len(test_set)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlDANqzorZUM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the data as input for the KoBERT model\n",
    "According tot he documentation the class BERTDataset is to be used to perform in the background the following tasks.\n",
    "- Tokenization\n",
    "- Numericalization (encoding string to integer)\n",
    "- Padding\n",
    "- etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwkUQs6vsdUc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of BERTDataset class (mandatory)\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXCfMA95suXZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setting the hyperparameters\n",
    "max_len = 64 # The maximum sequence length that this model might ever be used with. \n",
    "             # Typically set this to something large just in case (e.g., 512 or 1024 or 2048).\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10   # only parameter changed from 5 to 10 compared to the documentation\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5  # 4e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7PpEmMrtqyy",
    "outputId": "be05adb0-b69b-451a-a1e8-84445a6d6579",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Perform the prearation task of the data using class defined above\n",
    "bertmodel, vocab = get_pytorch_kobert_model() # calling the bert model and the vocabulary\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "train_set = BERTDataset(train_set, 0, 1, tok, max_len, True, False)\n",
    "val_set = BERTDataset(val_set, 0, 1, tok, max_len, True, False)\n",
    "#test_set = BERTDataset(test_set, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uX6eM0iJ9ViI",
    "outputId": "383f59d5-5b78-456c-96fb-6a3421aefa2d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CX4M61U09XAk",
    "outputId": "efd10eec-68fa-472f-e407-333af6d735e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCdfPvTF0cY7",
    "outputId": "df76ebf6-0c85-4e1c-b305-4591d3a633cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# verifying the transformation\n",
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTauspEW0qn2",
    "outputId": "7141abb5-ad0c-422b-b231-30a330543e11",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating torch-type datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, num_workers=5)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, num_workers=5)\n",
    "#test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTeEXPl21Idt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creation of the KoBERT learing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MrJMeGq1Ojn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This class is from the GitHub repository and the documentation\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,   # since we are in binary classification we set the value 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SI1Q0j199ueq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7P5F8_p16qX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creation of the model\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHo2DlB6FTfC",
    "outputId": "b75c1f04-3a8a-433d-b09d-5b2742fdab50",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbpS_GFj2mxl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbUNw59s2_y8",
    "outputId": "46e2553c-0c8b-4660-9d1c-54dc334bd820",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# configuration f the optimizer and loss function\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx6A1Ku43HPl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the function to compute the accury of the model\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOCoun1sy6IP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(pred, label, threshold=0.5):\n",
    "    pred = (pred > threshold).astype('float32')\n",
    "    tp = ((pred == 1) & (label == 1)).sum()\n",
    "    fp = ((pred == 1) & (label == 0)).sum()\n",
    "    fn = ((pred == 0) & (label == 1)).sum()\n",
    "    \n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = 2 * recall * precision / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPwuYcUU36XN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the KoBERT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-7fzY8a4F2L",
    "outputId": "43c1d4b7-0947-4789-9a96-fce3edfe1e07",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Training code from the github library\n",
    "start_time = time()\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    # Training of the model with the train set\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    # evaluation of the model train on the test set\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        labe2 = label.cpu()\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "        \n",
    "        pred = out.detach()\n",
    "        pred = F.softmax(pred)\n",
    "        pred = pred[:, 1].cpu().numpy().tolist()\n",
    "        preds += pred\n",
    "        labels += label.cpu().numpy().tolist()\n",
    "        \n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    metrics = get_metrics(preds, labels)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    # print('ACCURACY 2 = ', accuracy_score(out, label))\n",
    "    print('Metrics: ', metrics)\n",
    "\n",
    "run_time = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u7JZ9Y83Nuk",
    "outputId": "bda88ffd-065d-4564-d712-311f5ef19095",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_time\n",
    "#224.96386766433716\n",
    "#0.9947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "test_acc = 0.0\n",
    "# evaluation of the model train on the test set\n",
    "model.eval()\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    labe2 = label.cpu()\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "\n",
    "    pred = out.detach()\n",
    "    pred = F.softmax(pred)\n",
    "    pred = pred[:, 1].cpu().numpy().tolist()\n",
    "    preds += pred\n",
    "    labels += label.cpu().numpy().tolist()\n",
    "    \n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "metrics = get_metrics(preds, labels)\n",
    "print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "# print('ACCURACY 2 = ', accuracy_score(out, label))\n",
    "print('Metrics: ', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qznl7H3I6ZIa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Model Training result</h2>\n",
    "\n",
    "From the previous training result, we can see that our KoBERT binary classification model reached **99.68%** of accuracy on the test set and **100**%  of accuracy on the train set."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Fl-QdJx5O30T"
   ],
   "name": "Korean Text classification with KoBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
